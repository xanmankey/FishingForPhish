## PREFACE (as my methodology is still evolving, this is still changing, TODO)
This is the procedure that I followed, for those that want to attempt to replicate my experiment. 
It's important to understand that due to the number of differences between computer builds and setups, exact replication will be difficult, but I went into as much detail as possible here to help with replicability as much as I could. This meant including a detailed methodology file (right here), 
as well as including datasets (images and .arff files), my .model files (which can be used to load up a WEKA classification algorithm; in regards to feature selection, 
I didn't change the default WEKA settings at all, so config files aren't necessary), and my model output files (for WEKA classification models and feature selection). 
These can be found in their respective directory in the repository. Also, as much as I tried to avoid it, I ended up having to manually review my datasets for error screenshots, as not every screenshot took successfully, and some images wouldn't load using Selenium WebDriver, so I was forced to make adjustments to my datasets accordingly. However, the datasets that I got (from Open Page Rank and PhishStats) fluctuate at a high level anyway, and a machine learning methodology encourages the application of new data, so even though this comes at the cost of exact replicability from step 1 (which wasn't feasible anyway), the final datasets are provided (after preprocessing and manual review) and I highly encourage either making adaptations to my methodology or using new data to train the models.
#### ðŸ›ˆ A word of caution before attempting to replicate my methodology, your data may be at risk if you don't take proper precautions, due to the nature of phishing websites. I used a VPN and VM in tandem to attempt to minimize possible security threats.
## MATERIALS
### SPECS
  - Windows 64bit version 21H2 + Ubuntu version 20.04 setup using VS Codespaces (for working away from my personal setup)
### NOTABLE SOFTWARE (other dependencies can be found in requirements.txt)
  - WEKA GUI (preprocessing, feature selection, and classification; .model files can be found in the corresponding config folder), in addition to the imagefilters, the   SMOTE oversampling filter, dataset-weights, and the chi-squared attribute eval weka packages.
  - Selenium Webdriver (for scraping HTML, verifying URLs, taking screenshots, and other request-based functionality)
  - flask (web framework; I wanted to implement Django, but I'd never used it before and didn't have the time to learn it for this project)
  - python-weka-wrapper3 (for adding machine learning functionality to the website)
  - imagehash (specifically the phash algorithm, to preprocess my images by using computer vision to search for exact duplicates)
  - CS50 IDE + VS Codespaces for development
  - Vercel (https://vercel.com/) for server hosting
  - MongoDB for noSQL database hosting
  - (Some other software that I'm currently using to revise my methodology: imagemagick and Beautiful Soup)
### DATASETS
  - 5000 phishing-5000 legitimate dataset (https://doi.org/10.17632/h3cgnj8hft.1)
  - 500 Phishing Screenshots for training the model (I acquired data from the PhishStats database (https://phishstats.info/). I only took screenshots of phishing sites with phish scores of 7 and up (where 10 is defined as "OMG PHISHING" by PhishStats) to help differentiate phishing sites from non-phishing sites. In hindsight, randomized sites may have been a better idea, as only using phishing sites with high phishscores may have included bias in my model and hurt my overall accuracy). Phash algorithms and manual review turned this dataset into 100 images.
  - 500 Legitimate screenshots for training the model (https://www.domcop.com/, Open Page Rank data) to mirror the 500 phishing screenshots (this includes websites ranked from 6.39/10 to 10/10 by Open Page Rank, where Google Page Rank, the Page Rank that Open Page Rank is filling in for in it's stead, defined a 10 as "one of the most authoritative sites on the web"). Phash algorithms and manual review turned this dataset into 200 images.
## METHODS
1. Collect all necessary data (the datasets are included in the GitHub repository; credit goes to the databases and researchers who created them). 
When I was using webscreenshot to acquire the visuals necessary for my research, not all URLs would screenshot (I got the error "Shell command somehow failed" for a number of them, which I attribute to my use of the PhantomJS renderer, although I could be wrong. I still had more than enough data to work with though, it just meant that my image datasets were more varied). For the Legitimate image dataset, I had to use Selenium WebDriver to confirm URLs using the method driver.current_url, as I was only able to get a list of domains from Open Page Rank.
2. For the image datasets, run a perceptual hash filter with a threshold of 0 (using the imagehash python library) to check for exact duplicates, 
at least according to computer vision (the pre-imagehash datasets, as well as the post imagehash datasets are included in the github. 
If you want to skip this step, you can just use the pHash'd dataset of 194 screenshots). After running imagefilters, I ran the SMOTE filter (default settings) to create synthetic phishing instances to deal with oversampling in my data and balance the classes (SMOTE generates attributes by using the data and the nearest neighbors algorithm).
3. Create corresponding .arff files (I used python directory and file reading and writing as well as string replacement to automate the process). 
You need one .arff file for the page-based dataset and one for the image-based dataset (both with a legitimate and phishing class).
4. Run the Correlational, Information Gain, and Chi-squared filters on the page-based dataset and average out the top features (I chose these feature-selection algorithms because they are traditional algorithms and balance each other well when selecting features. I chose to analyze the top top 10 for each feature selection method, as it wouldn't have been feasible to examine every single feature, especially due to the mass quantity of attributes generated by the imagehash package). Then, take the top 10 features from each selector and run PageBasedFS.py, which gets you the top 7 features (however, I didn't use PctExtNullSelfRedirectHyperlinksRT, as I didn't understand it well enough. It seemed like it didn't convey enough information about the website, the information isn't obviously interpreted, and to me it seems like the information kind of repeats that of PctExtNullSelfRedirectHyperlinks anyway). 
5. Run the FCTHFilter, ColorLayoutFilter, EdgeHistogramFilter, and the BinaryPatternsPyramidFilter on the image-based datasets (I chose these 4 filters because they covered the texture and color layout analysis portions of computer vision well). Then, take the top 10 features for each feature from each selector and run ImageBasedFS.py to get the top 27 features (26 if PctExtNullSelfRedirectHyperlinksRT is removed).
6. Run the JRIP, Naive Bayes, and J48 models on both datasets (.models for all 3 classification models are provided in models/<dataset>/configs. I chose these 3 algorithms as they are informational algorithms that are easy to interpret and also tend to have high accuracy)
7. I decided to combine page-based and image-based techniques for my final dataset (I automated this process by using Selenium Web Driver and python weka wrapper 3 to create a new dataset with data from the legitimate and phishing websites; this process can be found in Datasets/CombinedMethodsDatset/DataCombinationScript/databuild.py and includes the use of filters such as the ImageFilters package and SMOTE used before, just in code form; also there are two scripts, one for the LegitimateData and one for the PhishingData). Unfortunately, it was at this stage of my procedure that I learned that I couldn't scrape HTML from a few websites, I think due to the nature of the webpage loading. This problem was approached by combining the Wayback Machine Firefox extension with Selenium Webdriver, which redirected the Webdriver to a saved version of a page (if possible) in the case of a 404 error. However, the problem still persisted, so the final dataset was split into 2: an original dataset and a dataset that makes use of the WEKA SMOTE filter to balance the classes (with a percentage of 336% being used).
8. Once the dataset has been properly combined, run the JRIP, Naive Bayes, and J48 models to get the results for the combined dataset (.models can be found in the corresponding model directory)
9. (the website described in this step is kind of finished, but it's reliant on key earlier decisions in the methodology, so I can't move forward with this until I finalize those) Now that you have all your models and data, you can test them. I programmed and hosted a website (PhishAI) on Vercel (https://vercel.com/) where the model can be tested on a URL of choice. Data will be outputted, including a screenshot of the webpage, HTML and image data, and the classification of the webpage based on the resulting data (or possible errors, if it turns out there are any). The methodology for the website isn't 100% faithful to that for my datasets, for example for the feature selection process I only ran the InformationGain filter and took the top 5 attributes from each filter that way, as iterating through and averaging was computationally slow in the context of a website. TODO: A MongoDB database of the data from the website (containing base64 encoding of the screenshots as well as feature data for the ranked attributes) is dynamically updated and available for download on the GitHub. Not all URLs will work, unfortunately, (TODO: and there may be issues regarding screenshotting and scraping due to the loading process of the website, but most will work). The website can be found here: (TODO)
