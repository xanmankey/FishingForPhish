# DNF
## PREFACE
This is the exact procedure that I followed, for those that want to attempt to replicate my experiment. 
It's important to understand that due to the number of differences between computer builds and setups, exact replication will be difficult, but I went into as much detail as possible here to help with replicability as much as I could. This meant including a detailed methodology file (right here), 
as well as including datasets (images and .arff files), my .model files (which can be used to load up a WEKA classification algorithm; in regards to feature selection, 
I didn't change the default WEKA settings at all, so config files aren't necessary), and my model output files (for WEKA classification models and feature selection). 
These can be found in their respective directory in the repository.
#### ðŸ›ˆ A word of caution before attempting to replicate my methodology, your data may be at risk if you don't take proper precautions, due to the nature of phishing websites
## MATERIALS
### SPECS
  - Windows 64bit version 21H2 + Ubuntu version 20.04 setup using VS Codespaces (for working away from my personal setup)
### NOTABLE SOFTWARE (other dependencies can be found in requirements.txt)
  - WEKA GUI (preprocessing, feature selection, and classification; .model files can be found in the corresponding config folder), in addition to the imagefilters, the   SMOTE oversampling filter, dataset-weights, and the chi-squared attribute eval weka packages.
  - Selenium Webdriver (for scraping HTML, verifying URLs, and other request-based functionality)
  - webscreenshot (automates the screenshot process using the PhantomJS renderer; unfortunately due to the nature of some websites or slow load times, I am not able to   successfully take a screenshot of every website due to an enforced timeout of 1.8 seconds. If you want a higher success rate, it's possible that increasing the timeout with the command line argument --ajax-max-timeouts could help, but I chose to leave it as the recommended default, as I didn't want to sacrifice the speed of the website).
  - flask (web framework; I wanted to implement Django, but I'd never used it before and didn't have the time to learn it for this project)
  - python-weka-wrapper3 (for adding machine learning functionality to the website)
  - imagehash (specifically the phash algorithm, to preprocess my images by using computer vision to search for exact duplicates)
  - CS50 IDE + VS Codespaces for development
  - Vercel (https://vercel.com/) for server hosting
### DATASETS
  - 5000 phishing-5000 legitimate dataset (https://doi.org/10.17632/h3cgnj8hft.1)
  - 500 Phishing Screenshots for training the model (I acquired data from the PhishStats database (https://phishstats.info/) on February 16th, and only took screenshots of phishing sites with phish scores of 7 and up to help differentiate phishing sites from non-phishing sites. In hindsight, randomized sites may have been a better idea, as only using phishing sites with high phishscores may have included bias in my model and hurt my overall accuracy). Phash algorithms and manual review turned this dataset into 100 images.
  - 500 Legitimate screenshots for training the model (https://www.domcop.com/, Open Page Rank data). Phash algorithms and manual review turned this dataset into 200 images.
## METHODS
1. Collect all necessary data (the datasets are included in the GitHub repository; credit goes to the databases and researchers who created them). 
When I was using webscreenshot to acquire the visuals necessary for my research, not all URLs would screenshot (I got the error "Shell command somehow failed" for a number of them, which I attribute to my use of the PhantomJS renderer, although I could be wrong. I still had more than enough data to work with though, it just meant that my image datasets were more varied). 
2. For the image datasets, run a perceptual hash filter with a threshold of 0 (using the imagehash python library) to check for exact duplicates, 
at least according to computer vision (the pre-imagehash datasets, as well as the post imagehash datasets are included in the github. 
If you want to skip this step, you can just use the pHash'd dataset of 194 screenshots). After running imagefilters, I ran the SMOTE filter (default settings) to create synthetic phishing instances to deal with oversampling in my data and balance the classes (SMOTE generates attributes by using the data and the nearest neighbors algorithm).
3. Create corresponding .arff files (I used python directory and file reading and writing as well as string replacement to automate the process). 
You need one .arff file for the page-based dataset and one for the image-based dataset (both with a legitimate and phishing class).
4. Run the Correlational, Information Gain, and Chi-squared filters on the page-based dataset and average out the top features (I chose these feature-selection algorithms because they are traditional algorithms and balance each other well when selecting features. I chose to analyze the top top 10 for each feature selection method, as it wouldn't have been feasible to examine every single feature, especially due to the mass quantity of attributes generated by the imagehash package). Then, take the top 10 features from each selector and run PageBasedFS.py, which gets you the top 7 features (however, I didn't use PctExtNullSelfRedirectHyperlinksRT, as I didn't understand it well enough. It seemed like it didn't convey enough information about the website, the information isn't obviously interpreted, and to me it seems like the information kind of repeats that of PctExtNullSelfRedirectHyperlinks anyway). 
5. Run the FCTHFilter, ColorLayoutFilter, EdgeHistogramFilter, and the BinaryPatternsPyramidFilter on the image-based datasets (I chose these 4 filters because they covered the texture and color layout analysis portions of computer vision well). Then, take the top 10 features for each feature from each selector and run ImageBasedFS.py to get the top 27 features.
6. Use the dataset-weights package to create an evenly weighted dataset combining all features as well as the selected features (the larger dataset is useful for machine learning and accuracy purposes, the smaller dataset is useful for feature analysis and accounts for computational limitations).
7. Run the JRIP, Naive Bayes, and J48 models on both datasets (.models for all 3 classification models are provided in models/<dataset>/configs. I chose these 3 algorithms as they are informational algorithms that are easy to interpret and also tend to have high accuracy)
8. I decided to combine page-based and image-based techniques for my final dataset (I automated this process by using Selenium Web Driver and python weka wrapper 3 to create a new dataset with data from the legitimate and phishing websites; this process can be found in Datasets/CombinedMethodsDatset/DataCombinationScript/databuild.py and includes the use of filters such as the ImageFilters package and SMOTE used before, just in code form; also there are two scripts, one for the LegitimateData and one for the PhishingData). Unfortunately, it was at this stage of my procedure that I learned that I couldn't scrape HTML from a few websites, I think due to the nature of the webpage loading. These include (INSERT LIST FROM TODO) and were replaced with (INSERT LIST FROM TODO).
9. Once the dataset has been properly combined, run the JRIP, Naive Bayes, and J48 models to get the results for the combined dataset (.models can be found in the corresponding model directory)
10. Now that you have all your models and data, you can test them. I programmed and hosted a website (PhishAI) on Vercel (https://vercel.com/) where the model can be tested on a URL of choice. Data will be outputted, including a screenshot of the webpage, HTML and image data, and the classification of the webpage based on the resulting data. Not all URLs will work, unfortunately, (TODO: and there may be issues regarding screenshotting and scraping due to the loading process of the website, but most will work). The website can be found here: (TODO)
